{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_cities(Dataframe,df):\n",
    "#     df[\"Areas\"].str.lower()\n",
    "#     Data_of_region =  df[\"Areas\"].values.tolist()\n",
    "#     Data_of_region = [each_city.lower() for each_city in Data_of_region]\n",
    "#     indexs = []\n",
    "#     notindexes = []\n",
    "#     for i in range(len(Dataframe)):\n",
    "#         doc = nlp(Dataframe[i][0])\n",
    "        \n",
    "#         city_found = False\n",
    "#         for city in doc.ents:\n",
    "#             city = str(city).lower()\n",
    "#             if city in Data_of_region:\n",
    "#                 indexs.append(i)\n",
    "#                 city_found = True\n",
    "#         if city_found == False:\n",
    "#             notindexes.append(i)\n",
    "            \n",
    "#     return notindexes,indexs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = datetime.date.today()\n",
    "path = str(todays_date.year)\n",
    "General_CAT = [\"front-page\", \"back-page\",\n",
    "               \"national\", \"business\", \"international\", \"sport\"]\n",
    "Metro_CAT = [\"karachi\", \"lahore\", \"islamabad\", \"peshawar\"]\n",
    "files = []\n",
    "Previous_Date = todays_date - datetime.timedelta(days=1)\n",
    "Previous_Date = Previous_Date.strftime('%Y-%m-%d')\n",
    "for i in range(len(General_CAT)):\n",
    "    files.append(path+\"/\"+str(Previous_Date)+\"/\"+General_CAT[i]+\".csv\")\n",
    "for i in range(len(Metro_CAT)):\n",
    "    files.append(path+\"/\"+str(Previous_Date)+\"/\"+Metro_CAT[i]+\".csv\")\n",
    "\n",
    "df = pd.read_csv(files[0], header=0)\n",
    "df = df.iloc[:, 1:]\n",
    "for i in range(len(files)-1):\n",
    "    read_ = pd.read_csv(files[i+1],header=0)\n",
    "    read_ = read_.iloc[:, 1:]\n",
    "    df = pd.concat([df, read_])\n",
    "Dataframe = df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = nlp(doc)\n",
    "    tokens = [tokens.lower_ for tokens in doc]\n",
    "    tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
    "    tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
    "    final_token = [WordNetLemmatizer().lemmatize(token.text)\n",
    "                   for token in tokens]\n",
    "\n",
    "    return \" \".join(final_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentences\n",
    "def sentences(text):\n",
    "    # split sentences and questions\n",
    "    text = re.split('[.?]', text)\n",
    "    clean_sent = []\n",
    "    for sent in text:\n",
    "        clean_sent.append(sent)\n",
    "    return clean_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlocations():\n",
    "    # Loading dataset \n",
    "    df = pd.read_csv('Alldata_refined.csv')\n",
    "    # Droping NULL rows\n",
    "    df = df.dropna()\n",
    "    # Extracting location col\n",
    "    df = df[\"Locations\"]\n",
    "    # Converting Data frame to sorted list in lower case\n",
    "    Data_of_region = df.values.tolist()\n",
    "    Data_of_region = [each_city.lower() for each_city in Data_of_region]\n",
    "    Data_of_region = list(dict.fromkeys(sorted(Data_of_region)))\n",
    "    # Storing indexes of each alphabet starting index \n",
    "    index = dict()\n",
    "    # Helping variables to store indexes \n",
    "    flag= False\n",
    "    push = False\n",
    "    current_alphabet = \"\"\n",
    "    start = 0\n",
    "    finish = 0\n",
    "    # Creating index hash \n",
    "    for i in range(len(Data_of_region)):\n",
    "        if i != 0 and Data_of_region[i][0] != Data_of_region[i][0]:\n",
    "            flag = True\n",
    "            push = True\n",
    "            finish = i-1\n",
    "        if push == True:\n",
    "            index[current_alphabet] = finish\n",
    "        if flag == False:\n",
    "            start = i\n",
    "            current_alphabet = Data_of_region[i][0]\n",
    "            index.__setitem__(current_alphabet, i)\n",
    "    return index,Data_of_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_location(read_more,header):\n",
    "    index, Data_of_region = getlocations()\n",
    "    header = header.lower()\n",
    "    header = clean(header)\n",
    "    header = header.split()\n",
    "    text = sentences(clean(read_more))\n",
    "    cities = dict()\n",
    "    for i in text:\n",
    "        doc = nlp(i)\n",
    "        for token in range(len(doc)):\n",
    "                if doc[token].pos_ == \"PROPN\":\n",
    "                    flag = False\n",
    "                    end = index[doc[token].text.lower()[0]]\n",
    "                    if end == index['a']:\n",
    "                        start = 0\n",
    "                    else:\n",
    "                        start = chr(ord(doc[token].text.lower()[0])-1)\n",
    "                        while(True):\n",
    "                            if start in index: \n",
    "                                start = index[start]-1\n",
    "                                break\n",
    "                            else:\n",
    "                                start = chr(ord(start)-1)\n",
    "                    area_count = 0\n",
    "                    for areas in range(start,end):\n",
    "                        words = Data_of_region[areas].split()\n",
    "                        subtoken = token\n",
    "                        checker = []\n",
    "                        for iterator in range(len(words)):\n",
    "                            if subtoken + iterator < len(doc):\n",
    "                                if doc[subtoken+iterator].text.lower() == words[iterator]:\n",
    "                                    checker.append(1)\n",
    "                        if len(checker) == len(words):\n",
    "                            city = ' '.join(words)\n",
    "                            area_count = len(words[iterator])\n",
    "                            flag = True\n",
    "                            break\n",
    "                    if flag:\n",
    "                        match = False\n",
    "                        word1 = \"\"\n",
    "                        word2 = \"\"\n",
    "                        if token != 0:\n",
    "                            word1 = doc[token-1].text.lower()\n",
    "                        subtoken = token + area_count\n",
    "                        if subtoken + 1 < len(doc):\n",
    "                            word2 = doc[subtoken+1].text.lower()\n",
    "                        if word1 in header or word2 in header:\n",
    "                            match = True\n",
    "                        if city in cities:\n",
    "                                if match:\n",
    "                                    cities[city] += 3\n",
    "                                else:\n",
    "                                    cities[city] += 1\n",
    "                        else:\n",
    "                            if match:\n",
    "                                cities.__setitem__(city, 3)\n",
    "                            else:\n",
    "                                cities.__setitem__(city, 1)\n",
    "    return cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chamber': 1, 'balochistan': 1, 'khyber pakhtunkhwa': 1}\n"
     ]
    }
   ],
   "source": [
    "# header = \"High-level flood in Sukkur Barrage as casualties rise by 57\"\n",
    "# header=\"123\"\n",
    "# print(Dataframe[0][0])\n",
    "# print(Dataframe[0][2])\n",
    "print(Get_location(Dataframe[10][2], Dataframe[10][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/muhammad/FYP/Data_Generator/Parser.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/muhammad/FYP/Data_Generator/Parser.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(Dataframe[\u001b[39m11\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/muhammad/FYP/Data_Generator/Parser.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(Dataframe[\u001b[39m11\u001b[39m][\u001b[39m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "print(Dataframe[11][0])\n",
    "print(Dataframe[11][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\": {\n",
      "        \"Creation_Date\": \"2022-09-12\",\n",
      "        \"Header\": {\n",
      "            \"Text\": \"Flood response goes digital to ensure transparency\"\n",
      "        },\n",
      "        \"Summary\": {\n",
      "            \"Text\": \"The dashboard will provide direct information to the public about the financial support, relief goods being received and distributed.\"\n",
      "        },\n",
      "        \"Details\": {\n",
      "            \"Text\": \"ISLAMABAD: In a bid to ensure transparency in the allocation of relief funds for the flood survivors, the federal government has decided to establish a \\u2018Digital Flood Dashboard\\u2019, which will keep the public abreast of relief measures taken by the authorities concerned.\\nThe dashboard prepared with the help of latest technology will be launched by Planning Minister and National Flood Response and Coordination Centre chief Ahsan Iqbal on Monday (today) following the directives of Prime Minister Shehbaz Sharif.\\nIt will provide direct information to the general public about the financial support and the relief goods being received and distributed among the flood-affected people across the country.\\nThe prime minister would personally supervise the digital dashboard.\\n\\nPM Shehbaz expresses gratitude to Erdogan, UAE\\u2019s minister\\n\\nThe premier had earlier annou\\u00adnced that the audit of financial aid received for flood victims would be conducted by the auditor general of Pakistan and a reputed audit firm.\\nPM Sharif reviewed measures aim\\u00aded at restoring life activities in the flood-affected areas and directed exp\\u00adediting the rehabilitation of roa\\u00adds, bridges, power supply, and other relief measures. He also dir\\u00adected that provision of basic necessities of life, including food items, should be ensured in the affected areas.\\nWhile stressing collective mechanisms and cooperation with provincial governments, he underlined the need for an early restoration of roads and the formulation of a comprehensive strategy to resolve other issues faced by the provinces.\\nPM talks to Erdogan\\nPrime Minister Sharif thanked United Arab Emirates Minister for Culture, Youth and Community Development Sheikh Nahyan bin Mubarak Al Nahyan for his generous donation of $10 million to help the flood victims.\\nSimilarly, he also expressed his gratitude to President Erdogan and the people of Turkiye for extending humanitarian relief assistance to Pakistan in the form of immediate dispatch of tents, emergency food items, and medicines via 12 Turkish military aircraft, four trains, and two Red Crescent trucks.\\nIn a telephonic conversation with Turkish President Erdogan, Mr Sharif said \\u201cas per initial estimates, the floods are likely to reduce Pakistan\\u2019s GDP by over two per cent.\\nHe said Pakistan was grappling with the immediate challenge of aver\\u00adting imminent food insecurity in the country.\\nPublished in Dawn, September 12th, 2022\\n\",\n",
      "            \"Tags\": [\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-12\",\n",
      "                    \"start\": 438,\n",
      "                    \"end\": 444,\n",
      "                    \"text\": \"Monday\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-09-12\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-12\",\n",
      "                    \"start\": 446,\n",
      "                    \"end\": 451,\n",
      "                    \"text\": \"today\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-09-12\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"focusTime\": \"2022-09-12\",\n",
      "        \"Locations\": {\n",
      "            \"islamabad\": 1\n",
      "        }\n",
      "    },\n",
      "    \"1\": {\n",
      "        \"Creation_Date\": \"2022-09-12\",\n",
      "        \"Header\": {\n",
      "            \"Text\": \"Rajapaksa leads Sri Lanka to Asia Cup glory\"\n",
      "        },\n",
      "        \"Summary\": {\n",
      "            \"Text\": \"DUBAI: Sri Lanka were crowned Asia Cup champions for the sixth time on Sun\\u00adday after Bhanuka Rajapa\\u00adksa\\u2019s ...\",\n",
      "            \"Tags\": [\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-11\",\n",
      "                    \"start\": 71,\n",
      "                    \"end\": 78,\n",
      "                    \"text\": \"Sun\\u00adday\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-09-11\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"Details\": {\n",
      "            \"Text\": \"DUBAI: Sri Lanka were crowned Asia Cup champions for the sixth time on Sun\\u00adday after Bhanuka Rajapa\\u00adksa\\u2019s batting pyrotechnics and Wanindu Hasaranga\\u2019s all-round brilliance secured them a 23-run victory against Pakistan in the final at the Dubai International Stadium.\\nRajapaksa smashed an unbeaten 71 to help Sri Lanka weather a top order collapse and reach 170-6 in their 20 overs.\\nWhen they returned, Hasaranga claimed three wickets in one over and Pramod Madushan claimed a career-best 4-34 to bowl out Pakistan for 147 in exactly 20 overs.\\nPakistan won the toss and skipper Babar Azam had no hesitation in fielding first since the side batting second had won nine of the 12 matches in the tournament.\\nAt one stage Sri Lanka were reeling at 58-5 in the ninth over after their top order wilted early.\\nNaseem Shah began with a wide, but redeemed himself on the third delivery by uprooting Kusal Mendis\\u2019s off-stump for a first-ball duck with a searing inswinger.\\nHaris Rauf removed Pat\\u00adhum Nissanka and Danu\\u00adshka Gunathilaka in successive overs while Iftikhar Ahmed took a sharp return catch to cut short Dhanan\\u00adjaya de Silva\\u2019s knock of 28.\\nPathum Nissanka attempted to play his shots in the first six overs of power play, but miscued Rauf and was caught at mid-off.\\nRauf sent the off stump cartwheeling in his second over to send Danushka Gunathilaka trudging back to the pavilion as Sri Lanka slipped to 36-3.\\nWickets kept tumbling with De Silva\\u2019s innings cut short by Iftikhar Ahmed on 28 and skipper Dasun Shanaka bowled by leg-spinner Shadab Khan on two as a packed crowd, dominated by Pakistani, went delirious.\\nWith Sri Lanka 67-5 at the halfway stage of their innings, Hasaranga (36) combined with Rajapaksa in their most productive partnership of 58 runs from 36 balls.\\nRauf (3-29) removed Hasaranga, but Rajapaksa remained unbeaten after an incendiary knock that included three sixes and six fours.\\nRajapaksa, who finished the innings with a four and six off Naseem in his 45-ball blitz, put on 54 runs with Chamika Karunaratne to further boost the total.\\nIt was largely because of his scintillating knock that Sri Lanka plundered 103 runs from the last 10 overs.\\nHaris Rauf stood out with figures of 3-29 off his four overs of pace.\\nEarly shocks\\nWhen Pakistan batted, Madushan claimed two wickets in two balls to rock the opposition.\\nBabar Azam was caught at short fine leg, while Fakhar Zaman dragged the next ball onto his stumps.\\nMadushan returned to dismiss Iftikhar (32) after the batsman had combined with Mohammad Rizwan (55) to revive their innings.\\nHasaranga effectively derailed Pakistan\\u2019s chase when the spinner dismissed the dangerous Rizwan, Asif Ali and Khushdil Shah in the same over.\\nThe victory would taste even better for Sri Lanka, who could not host the tournament at home because of a political and economic crisis in the island nation, though they retained the hosts\\u2019 honour.\\nScoreboard\\nSRI LANKA: \\nP. Nissanka c Babar b Haris8 \\nK. Mendis b Naseem0 \\nD. de Silva c&b b Iftikhar28 \\nD. Gunathilaka b Haris1 \\nB. Rajapaksa not out71 \\nD. Shanaka b Shadab2 \\nW. Hasaranga c Rizwan b Rauf36  \\nC. Karunaratne not out14 \\nEXTRAS (LB-7, W-2, B-1)10 \\nTOTAL (for six wickets, 20 overs)170\\nFALL OF WICKETS: 1-2 (Mendis), 2-23 (Nissanka), 3-36 (Gunathilaka), 4-53 (de Silva), 5-58 (Shanaka), 6-116 (de Silva)\\nDID NOT BAT:  M. Theekshana, P. Madushan, D. Madushanka \\nBOWLING: Naseem 4-0-40-1 (1w), Hasnain 4-0-41-0 (1w), Haris 4-0-29-3, Shadab 4-0-28-1, Iftikhar 3-0-21-1, Nawaz 1-0-3-0\\nPAKISTAN: \\nMohammad Rizwan c Gunathilaka b Hasaranga55 \\nBabar Azam c Madushanka b Madushan5 \\nFakhar  Zaman b Madushan0 \\nIftikhar Ahmed c sub (Bandara) b Madushan32  \\nMohammad Nawaz c Madushan b Karunaratne6 \\nKhushdil Shah c Theekshana b Hasaranga2 \\nAsif Ali b Hasaranga0  \\nShadab Khan c Gunathilaka b Theekshana8 \\nHaris Rauf b Karunaratne13 \\nNaseem Shah c Karunaratne b Madushan4 \\nMohammad Hasnain not out8 \\nEXTRAS (NB-2, W-12)14  \\nTOTAL (all out, 20 overs)147 \\nFALL OF WICKETS: 1-22 (Babar), 2-22 (Fakhar), 3-93 (Iftikhar), 4-102 (Nawaz), 5-110 (Rizwan), 6-111 (Asif), 7-112 (Khushdil) 8-120 (Shadab), 9-125 (Naseem) \\nBOWLING: Madushanka 3-0-24-0 (4w, 1nb), Theekshana 4-0-25-1 (1w), Madushan 4-0-34-4 (3w, 1nb), Hasaranga 4-0-27-3, Karunaratne 4-0-33-2, de Silva 1-0-4-0\\nRESULT: Sri Lanka won by 23 runs.\\nPublished in Dawn, September 12th, 2022\\n\"\n",
      "        },\n",
      "        \"focusTime\": \"2022-09-11\",\n",
      "        \"Locations\": {\n",
      "            \"asia\": 1,\n",
      "            \"babar\": 2\n",
      "        }\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"Creation_Date\": \"2022-09-12\",\n",
      "        \"Header\": {\n",
      "            \"Text\": \"Yet to join contempt probe, Imran holds another telethon\"\n",
      "        },\n",
      "        \"Summary\": {\n",
      "            \"Text\": \"Rs4.5bn pledged for flood relief; Fawad says PTI wants \\\"institutional balance\\\".\"\n",
      "        },\n",
      "        \"Details\": {\n",
      "            \"Text\": \"ISLAMABAD: Former prime minister Imran Khan on Sunday skipped another chance to join the investigation into the contempt charge he faces for his remarks targeting an Islamabad judge. \\nLater on Sunday night, the PTI chief hosted a second fundraiser to collect donations from North America for flood survivors.\\nAccording to party leader Faisal Javed, Pakistani nationals including expatriates had committed to donating over Rs4.5 billion to the fund started by Mr Khan to help flood-hit people. \\nDuring his previous telethon, the PTI chief had \\u201ccollected Rs5 billion\\u201d in pledges, out of which the party claims Rs3bn have been received.\\nEarlier in the day, Mr Khan for a third time refused to appear before the joint investigation team probing a terrorism charge against him in connection with his controversial remarks about a judge, Zeba Chaudhry.\\n\\nRs4.5bn pledged for flood relief; Fawad says PTI wants \\u2018institutional balance\\u2019\\n\\nThe police had registered a terrorism case against the PTI chairman after he threatened the judge, alongside high-ups of the capital police, with consequences for sending his chief aide Shahbaz Gill on physical remand in spite of allegations of \\u201ccustodial torture\\u201d.\\nMr Khan has already skipped two hearings of the JIT and this was his third opportunity to join the investigation. \\nThe police had summoned Mr Khan on Sunday at 6pm in connection with the case registered against him at the Margalla police station on Aug 20, but he failed to show up.\\nMeanwhile, PTI leaders claimed he had already submitted a written response to the police wherein he refused to join the investigation or face the JIT. In his reply, Mr Khan\\u2019s lawyer claimed the \\u201cthreats\\u201d against the judge could not be dealt with under terrorism law.\\nTelethon pledges\\nDuring the transmission on Sunday night, a US-based Pakistani citizen pledged $10 million to the ex-premier\\u2019s fund, while a Florida-based Pakistani pledged $50,000 to the fund.\\nOne resident of Canada pledged to donate $25,000, while another pledged $100,000. An Attock-resident pledged Rs2.5m, while Muslim Care UK also pledged 30,000 pounds to the fund.\\nA former European Parliament lawmaker said 64,000 pounds would be donated to the funds, while anchorperson Imran Riaz Khan said he and his friends had decided to contribute Rs36 million to the fund.\\nDuring the telethon, the ex-PM said Pakistan was the most vulnerable country in the world to climate change for which joint efforts were needed to mitigate the impacts of global warming. About the donation pledged by Pakistanis at the last week telethon, Mr Khan said out of Rs5 billion, pledges of at least Rs3 billion have been materialised.\\nSania Nishtar, who is in charge of the funds, said data from the Ehsaas programme would be used to distribute the amount among the survivors and assured that there would be no discrimination.\\n\\u2018Institutional balance\\u2019\\nMeanwhile, PTI leader Fawad Chaudhry at a presser in Islamabad urged \\u2018unelected institutions\\u2019 to respect the mandate of political leadership as he sought \\u2018institutional balance\\u2019 to address the prevailing challenges. \\u201cThe ECP is also suffering from an administrative crisis and incompetence, be it constituencies or declaring four million voters dead, or now postponing the elections,\\u201d he said.\\nThe PTI wanted to have \\u201cclose ties with the institutions and want to be on the same page\\u201d but \\u201cunelected institutions\\u201d did not represent the people, he said, adding that the decisions should be taken by the people.\\nMr Chaudhry also questioned delays in the cases against Prime Minister Shehbaz Sharif.\\nPublished in Dawn, September 12th, 2022\\n\",\n",
      "            \"Tags\": [\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-11\",\n",
      "                    \"start\": 47,\n",
      "                    \"end\": 53,\n",
      "                    \"text\": \"Sunday\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-09-11\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-18TNI\",\n",
      "                    \"start\": 193,\n",
      "                    \"end\": 205,\n",
      "                    \"text\": \"Sunday night\",\n",
      "                    \"type\": \"TIME\",\n",
      "                    \"value\": \"2022-09-18TNI\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"PT1S\",\n",
      "                    \"start\": 228,\n",
      "                    \"end\": 236,\n",
      "                    \"text\": \"a second\",\n",
      "                    \"type\": \"DURATION\",\n",
      "                    \"value\": \"PT1S\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-12\",\n",
      "                    \"start\": 645,\n",
      "                    \"end\": 652,\n",
      "                    \"text\": \"the day\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-09-12\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-18T18:00\",\n",
      "                    \"start\": 1344,\n",
      "                    \"end\": 1357,\n",
      "                    \"text\": \"Sunday at 6pm\",\n",
      "                    \"type\": \"TIME\",\n",
      "                    \"value\": \"2022-09-18T18:00\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"2022-08-20\",\n",
      "                    \"start\": 1443,\n",
      "                    \"end\": 1449,\n",
      "                    \"text\": \"Aug 20\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-08-20\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-18TNI\",\n",
      "                    \"start\": 1788,\n",
      "                    \"end\": 1800,\n",
      "                    \"text\": \"Sunday night\",\n",
      "                    \"type\": \"TIME\",\n",
      "                    \"value\": \"2022-09-18TNI\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"2022-W37\",\n",
      "                    \"start\": 2546,\n",
      "                    \"end\": 2559,\n",
      "                    \"text\": \"the last week\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-W37\"\n",
      "                },\n",
      "                {\n",
      "                    \"timex-value\": \"PRESENT_REF\",\n",
      "                    \"start\": 3229,\n",
      "                    \"end\": 3232,\n",
      "                    \"text\": \"now\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"PRESENT_REF\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"focusTime\": \"2022-09-11\",\n",
      "        \"Locations\": {\n",
      "            \"islamabad\": 1\n",
      "        }\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"Creation_Date\": \"2022-09-12\",\n",
      "        \"Header\": {\n",
      "            \"Text\": \"\\u2018AQIS, TTP men\\u2019 captured in Karachi\"\n",
      "        },\n",
      "        \"Summary\": {\n",
      "            \"Text\": \"KARACHI: Law enforcement agencies arrested two suspected militants in different areas of Karachi on Sunday. One of...\",\n",
      "            \"Tags\": [\n",
      "                {\n",
      "                    \"timex-value\": \"2022-09-11\",\n",
      "                    \"start\": 100,\n",
      "                    \"end\": 106,\n",
      "                    \"text\": \"Sunday\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-09-11\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"Details\": {\n",
      "            \"Text\": \"KARACHI: Law enforcement agencies arrested two suspected militants in different areas of Karachi on Sunday. One of them admitted that he had been working for the outlawed Al Qaeda in the Indian Sub-continent (AQIS) and had received \\u2018training\\u2019 in Afghanistan.    \\nThe other was a member of the outlawed Tehreek-i-Taliban Pakistan\\u2019s notorious Tariq Gedar group and was allegedly involved in attacks on security agencies in Khyber Pakhtunkhwa. \\nSohaib Tariq alias Usman, the first suspect, was taken into custody after law enfor\\u00adcement agencies raided a hideout in Liaquatabad\\u2019s Super Market, according to the police.  \\nHe revealed during interrogation that an AQIS militant, Talha, had introduced him to the outfit\\u2019s chief in Afghanistan\\u2019s Bahram Chah region in 2019. Sohaib received training in militancy in Bahram Chah and met two Indian citizens, Abdul Rehman and Taha.  \\nHe returned to Pakistan after a few months and started working for the banned group\\u2019s quarterly magazine, Nawai Ghazwa-i-Hind, which was shared with the outfit\\u2019s \\u2018sympathisers\\u2019 on the Telegram app. He was also tasked with putting online the audio and video recordings of speeches delivered by AQIS leaders, Maulana Asim Umer, Sheikh Osama Mehmood and others.  \\nSohaib Tariq said he had gone underground after three of his accomplices were arrested. He identified them as Mohammed alias Arman, Abu Urwa and Abdul Manan.  \\nThe police said they had recovered one pistol, one hand-grenade, three mobile phones and an unspecified amount in cash from Sohaib.  \\nThe other suspect, Mohammad Shah alias Ding, was taken into custody by the Counter-Terrorism Department after a raid in Metroville, SITE. \\nMohammad Shah, who belonged to the outlawed TTP\\u2019s Tariq Gedar group, was \\u201cplanning to activate the TTP\\u2019s network in Karachi,\\u201d according to a CTD spokesperson. Security agencies blame the Gedar group for a rocket attack on a checkpoint of the Frontier Corps in Azakhail.  \\nOn July 17 this year, the spokesperson added, Mohammad Shah carried out an attack on police in Matna, leaving two cops \\u2014 Jan Ali and Akber \\u2014 martyred. \\n\\u201cShah was also involved in extorting money from several people in Darra Adam Khel.\\u201d\\nPublished in Dawn, September 12th, 2022\\n\"\n",
      "        },\n",
      "        \"focusTime\": \"2022-09-11\",\n",
      "        \"Locations\": {\n",
      "            \"khyber pakhtunkhwa\": 1,\n",
      "            \"bahram\": 1,\n",
      "            \"abdul rehman\": 1\n",
      "        }\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"Creation_Date\": \"2022-09-12\",\n",
      "        \"Header\": {\n",
      "            \"Text\": \"110 districts hit by floods as locals race to save Dadu\"\n",
      "        },\n",
      "        \"Summary\": {\n",
      "            \"Text\": \"45pc of country\\u2019s cropland washed away; nearly 1,200 killed since June 14; Johi, Mehar in Sindh cut off from other areas.\",\n",
      "            \"Tags\": [\n",
      "                {\n",
      "                    \"timex-value\": \"2022-06-14\",\n",
      "                    \"start\": 66,\n",
      "                    \"end\": 73,\n",
      "                    \"text\": \"June 14\",\n",
      "                    \"type\": \"DATE\",\n",
      "                    \"value\": \"2022-06-14\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"Details\": {\n",
      "            \"Text\": \"\\u2022 45pc of country\\u2019s cropland washed away, overall damages at $10bn\\n\\u2022 Nearly 1,200 killed since June 14\\n\\u2022 Johi, Mehar cut off from other areas\\n\\u2022 Met Office predicts more rains, flash floods this month\\nDADU / ISLAMABAD: Sindh braced for yet more flooding on Thursday as a surge of water flowed down the Indus river, leaving parts of Dadu district inundated.\\nMeanwhile, Climate Cha\\u00adnge Minister Sherry Reh\\u00adman told a summit that \\u201cmon\\u00adster\\u201d monsoon floods had washed away 45 per cent of country\\u2019s cropland, mai\\u00adnly in Sindh and caused around $10 billion in damages on the whole. In her estimation, around 70pc districts in the country are now under water. Overall, a third of Pakistan \\u2014 or an area roughly the size of the UK \\u2014 is inundated.\\nAccording to official data, the number of affected districts now stands at 110, including 34 in Balochistan, 33 in Khyber Pakhtunkhwa, 16 in Sindh and the rest in Punjab, Gilgit-Baltistan and Azad Kashmir.\\nIn Dadu, with Khairpur Nathan Shah city already sub\\u00admerged, residents of Johi and Mehar were racing aga\\u00adinst time to protect their cities.\\nThe 50,000 citizens of Johi and 10,000 flood-affected people who have come here from various flood-hit villages are under threat as water level was rising along with the ring embankment in Johi city.\\nRead: Pakistan's history of disasters and the lessons we fail to learn\\nTariq Rind, a resident of Johi, told Dawn that 600 villages of Dadu district had been submerged and many people were stranded amid a shortage of boats. They are also faced with food and water scarcity.\\nTo reduce pressure on the embankments of Johi and Dadu cities, the irrigation department has made a 1,000-foot-wide cut at the MNV drain close to Manchhar lake.\\nMen, women and children have gathered to work shifts to form new dikes and reinforce existing ones using sandbags, stones and other materials. Apart from fellow residents, songs blaring from loudspeakers are a major morale booster for those working tirelessly to put anything in the way of water ready to swallow vast swathes of the landscape.\\nDeath toll hits 1,191\\nThe floods have killed at least 1,191 people, including 399 children, with 21 deaths reported in the last 24 hours.\\nThe Met Office has predicted even more rains and flash floods this month. \\u201cOverall, a tendency for normal to above normal precipitation is likely over the country during September,\\u201d it said in a monthly outlook released on Thursday.\\nThe military said on Thursday it had evacuated some 50,000 people, including 1,000 by air, since rescue efforts began.\\n\\u201cWe\\u2019re on a high alert as water arriving downstream from northern flooding is expected to enter the province over the next few days,\\u201d Sindh provincial government spokesman Murtaza Wahab told Reuters news agency.\\nHe said a flow of some 600,000 cubic feet per second was expected to swell the Indus, testing its flood defences.\\nUnder 10 feet of water\\nIn Khairpur Nathan Shah, the water level has risen to 10 feet, leaving thousands of families homeless. Residents were roaming from one place to another place in search of food, according to Hafiz Amin Jamali, the president of the local citizens\\u2019 action committee.\\nWith the Indus Highway submerged, Mehar has also been cut off from other areas. Residents continued to raise the ring embankment level to protect the city.\\n\\u201cWe have been working to make and reinforce this dike since early morning,\\u201d Damshad Ali, 20, told Reuters, vowing to stay in the flood-stricken area with his family.\\nRead: How current calamity compares to 2010 floods\\nNearby, another man called for help. \\u201cI appeal to all young men to come join the dike strengthening, God willing we will save the city of Mehar from the floodwaters,\\u201d he said from a mound of sandbags as local residents joined the effort. Dadu city is also under threat from four sides.\\n45pc cropland washed away\\nThe government on Thursday said \\u201cmonster\\u201d monsoon floods had washed away 45pc of the country\\u2019s cropland, mainly in Sindh and caused $10bn in damages.\\nFederal Minister for Climate Change Sherry Rehman shared the figures at a summit jointly held by her ministry and Unicef in Islamabad to brief young leaders on the country\\u2019s vulnerability due to climate change.\\nShe said Sindh was the food basket of the country and its 45pc area was washed away due to heavy flooding, which would bring economic shocks in the future.\\n\\u201cMy generation had the carbon-intensive lifestyle and you will have to define at one level what we have to change in our life,\\u201d she told the young participants.\\n\\u201cNo such environmental and humanitarian crisis has occurred before and we should consider this as the decade\\u2019s major climate event. As 70pc of Pakistan\\u2019s (districts) are underwater because of a climate catastrophe, make no mistake that it\\u2019s all man-made disaster and it will not go back automatically,\\u201d she said.\\n\\u201cOur folklore and songs had been on monsoon season, which used to be of two to three spells, but this is a monster flood. Our helicopters could not get off the ground due to persistent rainfall and inclement weather,\\u201d she said.\\nPublished in Dawn, September 2nd, 2022\\n\"\n",
      "        },\n",
      "        \"focusTime\": \"2022-06-14\",\n",
      "        \"Locations\": {\n",
      "            \"mehar\": 2,\n",
      "            \"balochistan\": 1,\n",
      "            \"khyber pakhtunkhwa\": 1,\n",
      "            \"dadu\": 2,\n",
      "            \"khairpur\": 4,\n",
      "            \"johi\": 5,\n",
      "            \"jamali\": 1,\n",
      "            \"islamabad\": 1\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from unittest import result\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import json\n",
    "import readline\n",
    "from sutime import SUTime\n",
    "from datetime import datetime\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])\n",
    "\n",
    "\n",
    "class parser():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def clean(self, doc):\n",
    "        doc = doc.lower()\n",
    "        doc = nlp(doc)\n",
    "        tokens = [tokens.lower_ for tokens in doc]\n",
    "        tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
    "        tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
    "        final_token = [WordNetLemmatizer().lemmatize(token.text)\n",
    "                       for token in tokens]\n",
    "\n",
    "        return \" \".join(final_token)\n",
    "\n",
    "    # split sentences\n",
    "\n",
    "    def sentences(self,text):\n",
    "        # split sentences and questions\n",
    "        text = re.split('[.?]', text)\n",
    "        clean_sent = []\n",
    "        for sent in text:\n",
    "            clean_sent.append(sent)\n",
    "        return clean_sent\n",
    "\n",
    "    def load_cities(self, file):\n",
    "        # Loading dataset\n",
    "        df = pd.read_csv(file)\n",
    "        # Droping NULL rows\n",
    "        df = df.dropna()\n",
    "        # Extracting location col\n",
    "        df = df[\"Locations\"]\n",
    "        # Converting Data frame to sorted list in lower case\n",
    "        Data_of_region = df.values.tolist()\n",
    "        Data_of_region = [each_city.lower() for each_city in Data_of_region]\n",
    "        Data_of_region = list(dict.fromkeys(sorted(Data_of_region)))\n",
    "        # Storing indexes of each alphabet starting index\n",
    "        index = dict()\n",
    "        # Helping variables to store indexes\n",
    "        flag = False\n",
    "        push = False\n",
    "        current_alphabet = \"\"\n",
    "        start = 0\n",
    "        finish = 0\n",
    "        # Creating index hash\n",
    "        for i in range(len(Data_of_region)):\n",
    "            if i != 0 and Data_of_region[i][0] != Data_of_region[i][0]:\n",
    "                flag = True\n",
    "                push = True\n",
    "                finish = i-1\n",
    "            if push == True:\n",
    "                index[current_alphabet] = finish\n",
    "            if flag == False:\n",
    "                start = i\n",
    "                current_alphabet = Data_of_region[i][0]\n",
    "                index.__setitem__(current_alphabet, start)\n",
    "        self.index = index\n",
    "        self.Data_of_region = Data_of_region\n",
    "\n",
    "    def Get_location(self, read_more, header):\n",
    "        self.load_cities(r'Alldata_refined.csv')\n",
    "        header = header.lower()\n",
    "        header = self.clean(header)\n",
    "        header = header.split()\n",
    "        text = self.sentences(self.clean(read_more))\n",
    "        cities = dict()\n",
    "        for i in text:\n",
    "            doc = nlp(i)\n",
    "            for token in range(len(doc)):\n",
    "                if doc[token].pos_ == \"PROPN\":\n",
    "                    flag = False\n",
    "                    end = self.index[doc[token].text.lower()[0]]\n",
    "                    if end == self.index['a']:\n",
    "                        start = 0\n",
    "                    else:\n",
    "                        start = chr(ord(doc[token].text.lower()[0])-1)\n",
    "                        while (True):\n",
    "                            if start in self.index:\n",
    "                                start = self.index[start]-1\n",
    "                                break\n",
    "                            else:\n",
    "                                start = chr(ord(start)-1)\n",
    "                    area_count = 0\n",
    "                    for areas in range(start, end):\n",
    "                        words = self.Data_of_region[areas].split()\n",
    "                        subtoken = token\n",
    "                        checker = []\n",
    "                        for iterator in range(len(words)):\n",
    "                            if subtoken + iterator < len(doc):\n",
    "                                if doc[subtoken+iterator].text.lower() == words[iterator]:\n",
    "                                    checker.append(1)\n",
    "                        if len(checker) == len(words):\n",
    "                            city = ' '.join(words)\n",
    "                            area_count = len(words[iterator])\n",
    "                            flag = True\n",
    "                            break\n",
    "                    if flag:\n",
    "                        match = False\n",
    "                        word1 = \"\"\n",
    "                        word2 = \"\"\n",
    "                        if token != 0:\n",
    "                            word1 = doc[token-1].text.lower()\n",
    "                        subtoken = token + area_count\n",
    "                        if subtoken + 1 < len(doc):\n",
    "                            word2 = doc[subtoken+1].text.lower()\n",
    "                        if word1 in header or word2 in header:\n",
    "                            match = True\n",
    "                        if city in cities:\n",
    "                            if match:\n",
    "                                cities[city] += 3\n",
    "                            else:\n",
    "                                cities[city] += 1\n",
    "                        else:\n",
    "                            if match:\n",
    "                                cities.__setitem__(city, 3)\n",
    "                            else:\n",
    "                                cities.__setitem__(city, 1)\n",
    "        return cities\n",
    "\n",
    "    def checkDate(self, parsedData, referenceDate):\n",
    "        flag = False\n",
    "        for tag in parsedData:\n",
    "            if tag['type'] == 'DATE':\n",
    "                # if datetime.strptime(tag['value'], '%Y-%m-%d') <= datetime.strptime(referenceDate, '%Y-%m-%d'):\n",
    "                try:\n",
    "                    if datetime.strptime(tag['value'], '%Y-%m-%d') <= datetime.strptime(referenceDate, '%Y-%m-%d'):\n",
    "                        flag = True\n",
    "                except:\n",
    "                    pass\n",
    "        return flag\n",
    "\n",
    "    def extractDate(self, parsedData, referenceDate):\n",
    "        focusTime = ''\n",
    "        for tag in parsedData:\n",
    "            if tag['type'] == 'DATE':\n",
    "                try:\n",
    "                    if datetime.strptime(tag['value'], '%Y-%m-%d') <= datetime.strptime(referenceDate, '%Y-%m-%d'):\n",
    "                        focusTime = tag['value']\n",
    "                        return focusTime\n",
    "                except:\n",
    "                    pass   \n",
    "\n",
    "\n",
    "    def Get_time(self, document):\n",
    "        sutime = SUTime()\n",
    "        # print(json.dumps(sutime.parse(test_case), sort_keys=True, indent=4))\n",
    "        timeData = dict()\n",
    "        # print(str)\n",
    "        count = 0\n",
    "        for data in document.itertuples():\n",
    "            headerParse = sutime.parse(data[2], reference_date=data[5])\n",
    "            self.checkDate(headerParse, data[5])\n",
    "            if not headerParse or not self.checkDate(headerParse, data[5]):\n",
    "                summaryParse = sutime.parse(data[3], reference_date=data[5])\n",
    "                self.checkDate(summaryParse, data[5])\n",
    "                if not summaryParse or not self.checkDate(summaryParse, data[5]):\n",
    "                    details = data[4]\n",
    "                    lines = details.split('\\n')\n",
    "                    del lines[-2]\n",
    "                    details = '\\n'.join(lines)\n",
    "                    detailsParse = sutime.parse(details, reference_date=data[5])\n",
    "                    self.checkDate(detailsParse, data[5])\n",
    "                    if not detailsParse:\n",
    "                        count += 1\n",
    "                    else:\n",
    "                        timeData[data[0]] = dict()\n",
    "                        timeData[data[0]][\"Creation_Date\"] = data[5]\n",
    "                        timeData[data[0]][\"Header\"] = dict()\n",
    "                        timeData[data[0]][\"Header\"][\"Text\"] = data[2]\n",
    "                        timeData[data[0]]['Summary'] = dict()\n",
    "                        timeData[data[0]]['Summary'][\"Text\"] = data[3]\n",
    "                        timeData[data[0]]['Details'] = dict()\n",
    "                        timeData[data[0]]['Details'][\"Text\"] = data[4]\n",
    "                        timeData[data[0]]['Details'][\"Tags\"] = detailsParse\n",
    "                        timeData[data[0]][\"focusTime\"] = self.extractDate(detailsParse, data[5])\n",
    "                else:\n",
    "                    timeData[data[0]] = dict()\n",
    "                    timeData[data[0]][\"Creation_Date\"] = data[5]\n",
    "                    timeData[data[0]][\"Header\"] = dict()\n",
    "                    timeData[data[0]][\"Header\"][\"Text\"] = data[2]\n",
    "                    timeData[data[0]]['Summary'] = dict()\n",
    "                    timeData[data[0]]['Summary'][\"Text\"] = data[3]\n",
    "                    timeData[data[0]]['Details'] = dict()\n",
    "                    timeData[data[0]]['Details'][\"Text\"] = data[4]\n",
    "                    timeData[data[0]]['Summary'][\"Tags\"] = summaryParse\n",
    "                    timeData[data[0]][\"focusTime\"] = self.extractDate(summaryParse, data[5])\n",
    "            else:\n",
    "                timeData[data[0]] = dict()\n",
    "                timeData[data[0]][\"Creation_Date\"] = data[5]\n",
    "                timeData[data[0]][\"Header\"] = dict()\n",
    "                timeData[data[0]][\"Header\"][\"Text\"] = data[2]\n",
    "                timeData[data[0]]['Summary'] = dict()\n",
    "                timeData[data[0]]['Summary'][\"Text\"] = data[3]\n",
    "                timeData[data[0]]['Details'] = dict()\n",
    "                timeData[data[0]]['Details'][\"Text\"] = data[4]\n",
    "                timeData[data[0]][\"Header\"][\"Tags\"] = headerParse\n",
    "                timeData[data[0]][\"focusTime\"] = self.extractDate(headerParse, data[5])\n",
    "            if timeData[data[0]][\"focusTime\"] == 'null':\n",
    "                timeData[data[0]][\"focusTime\"] = data[5]\n",
    "        return timeData\n",
    "\n",
    "def main():\n",
    "    filename = r\"C:\\Danyal\\Work\\FAST\\Semester 7\\Final Year Project - I\\Dummy Project\\NAaaS\\Data_Generator\\2022\\2022-09-12\\front-page.csv\"\n",
    "    path = pathlib.PurePath(filename)\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, dtype=\"string\")\n",
    "    df['Creation_Date'] = path.parent.name\n",
    "    Parser = parser()\n",
    "    results = Parser.Get_time(df)\n",
    "\n",
    "    for data in df.itertuples():\n",
    "        result = Parser.Get_location(data[4], data[2])\n",
    "        results[data[0]][\"Locations\"] = result\n",
    "    print(json.dumps(results, indent=4))\n",
    "\n",
    "main()    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('DIP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fee64b6d10f71b2ad32d2aedca1959ec8ccaf90c3c6db5aa37e1a96c8621a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
