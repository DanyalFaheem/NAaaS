{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime \n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_cities(Dataframe,df):\n",
    "#     df[\"Areas\"].str.lower()\n",
    "#     Data_of_region =  df[\"Areas\"].values.tolist()\n",
    "#     Data_of_region = [each_city.lower() for each_city in Data_of_region]\n",
    "#     indexs = []\n",
    "#     notindexes = []\n",
    "#     for i in range(len(Dataframe)):\n",
    "#         doc = nlp(Dataframe[i][0])\n",
    "        \n",
    "#         city_found = False\n",
    "#         for city in doc.ents:\n",
    "#             city = str(city).lower()\n",
    "#             if city in Data_of_region:\n",
    "#                 indexs.append(i)\n",
    "#                 city_found = True\n",
    "#         if city_found == False:\n",
    "#             notindexes.append(i)\n",
    "            \n",
    "#     return notindexes,indexs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = datetime.date.today()\n",
    "path = str(todays_date.year)\n",
    "General_CAT = [\"front-page\", \"back-page\",\n",
    "               \"national\", \"business\", \"international\", \"sport\"]\n",
    "Metro_CAT = [\"karachi\", \"lahore\", \"islamabad\", \"peshawar\"]\n",
    "files = []\n",
    "Previous_Date = todays_date - datetime.timedelta(days=1)\n",
    "Previous_Date = Previous_Date.strftime('%Y-%m-%d')\n",
    "for i in range(len(General_CAT)):\n",
    "    files.append(path+\"/\"+str(Previous_Date)+\"/\"+General_CAT[i]+\".csv\")\n",
    "for i in range(len(Metro_CAT)):\n",
    "    files.append(path+\"/\"+str(Previous_Date)+\"/\"+Metro_CAT[i]+\".csv\")\n",
    "\n",
    "df = pd.read_csv(files[0], header=0)\n",
    "df = df.iloc[:, 1:]\n",
    "for i in range(len(files)-1):\n",
    "    read_ = pd.read_csv(files[i+1],header=0)\n",
    "    read_ = read_.iloc[:, 1:]\n",
    "    df = pd.concat([df, read_])\n",
    "Dataframe = df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = nlp(doc)\n",
    "    tokens = [tokens.lower_ for tokens in doc]\n",
    "    tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
    "    tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
    "    final_token = [WordNetLemmatizer().lemmatize(token.text)\n",
    "                   for token in tokens]\n",
    "\n",
    "    return \" \".join(final_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentences\n",
    "def sentences(text):\n",
    "    # split sentences and questions\n",
    "    text = re.split('[.?]', text)\n",
    "    clean_sent = []\n",
    "    for sent in text:\n",
    "        clean_sent.append(sent)\n",
    "    return clean_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getlocations():\n",
    "    # Loading dataset \n",
    "    df = pd.read_csv('Alldata_refined.csv')\n",
    "    # Droping NULL rows\n",
    "    df = df.dropna()\n",
    "    # Extracting location col\n",
    "    df = df[\"Locations\"]\n",
    "    # Converting Data frame to sorted list in lower case\n",
    "    Data_of_region = df.values.tolist()\n",
    "    Data_of_region = [each_city.lower() for each_city in Data_of_region]\n",
    "    Data_of_region = list(dict.fromkeys(sorted(Data_of_region)))\n",
    "    # Storing indexes of each alphabet starting index \n",
    "    index = dict()\n",
    "    # Helping variables to store indexes \n",
    "    flag= False\n",
    "    push = False\n",
    "    current_alphabet = \"\"\n",
    "    start = 0\n",
    "    finish = 0\n",
    "    # Creating index hash \n",
    "    for i in range(len(Data_of_region)):\n",
    "        if i != 0 and Data_of_region[i][0] != Data_of_region[i][0]:\n",
    "            flag = True\n",
    "            push = True\n",
    "            finish = i-1\n",
    "        if push == True:\n",
    "            index[current_alphabet] = finish\n",
    "        if flag == False:\n",
    "            start = i\n",
    "            current_alphabet = Data_of_region[i][0]\n",
    "            index.__setitem__(current_alphabet, i)\n",
    "    return index,Data_of_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_location(read_more,header):\n",
    "    index, Data_of_region = getlocations()\n",
    "    header = header.lower()\n",
    "    header = clean(header)\n",
    "    header = header.split()\n",
    "    text = sentences(clean(read_more))\n",
    "    cities = dict()\n",
    "    for i in text:\n",
    "        doc = nlp(i)\n",
    "        for token in range(len(doc)):\n",
    "                if doc[token].pos_ == \"PROPN\":\n",
    "                    flag = False\n",
    "                    end = index[doc[token].text.lower()[0]]\n",
    "                    if end == index['a']:\n",
    "                        start = 0\n",
    "                    else:\n",
    "                        start = chr(ord(doc[token].text.lower()[0])-1)\n",
    "                        while(True):\n",
    "                            if start in index: \n",
    "                                start = index[start]-1\n",
    "                                break\n",
    "                            else:\n",
    "                                start = chr(ord(start)-1)\n",
    "                    area_count = 0\n",
    "                    for areas in range(start,end):\n",
    "                        words = Data_of_region[areas].split()\n",
    "                        subtoken = token\n",
    "                        checker = []\n",
    "                        for iterator in range(len(words)):\n",
    "                            if subtoken + iterator < len(doc):\n",
    "                                if doc[subtoken+iterator].text.lower() == words[iterator]:\n",
    "                                    checker.append(1)\n",
    "                        if len(checker) == len(words):\n",
    "                            city = ' '.join(words)\n",
    "                            area_count = len(words[iterator])\n",
    "                            flag = True\n",
    "                            break\n",
    "                    if flag:\n",
    "                        match = False\n",
    "                        word1 = \"\"\n",
    "                        word2 = \"\"\n",
    "                        if token != 0:\n",
    "                            word1 = doc[token-1].text.lower()\n",
    "                        subtoken = token + area_count\n",
    "                        if subtoken + 1 < len(doc):\n",
    "                            word2 = doc[subtoken+1].text.lower()\n",
    "                        if word1 in header or word2 in header:\n",
    "                            match = True\n",
    "                        if city in cities:\n",
    "                                if match:\n",
    "                                    cities[city] += 3\n",
    "                                else:\n",
    "                                    cities[city] += 1\n",
    "                        else:\n",
    "                            if match:\n",
    "                                cities.__setitem__(city, 3)\n",
    "                            else:\n",
    "                                cities.__setitem__(city, 1)\n",
    "    return cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chamber': 1, 'balochistan': 1, 'khyber pakhtunkhwa': 1}\n"
     ]
    }
   ],
   "source": [
    "# header = \"High-level flood in Sukkur Barrage as casualties rise by 57\"\n",
    "# header=\"123\"\n",
    "# print(Dataframe[0][0])\n",
    "# print(Dataframe[0][2])\n",
    "print(Get_location(Dataframe[10][2], Dataframe[10][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/muhammad/FYP/Data_Generator/Parser.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/muhammad/FYP/Data_Generator/Parser.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(Dataframe[\u001b[39m11\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/muhammad/FYP/Data_Generator/Parser.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(Dataframe[\u001b[39m11\u001b[39m][\u001b[39m2\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "print(Dataframe[11][0])\n",
    "print(Dataframe[11][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('DIP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fee64b6d10f71b2ad32d2aedca1959ec8ccaf90c3c6db5aa37e1a96c8621a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
