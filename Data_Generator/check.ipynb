{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\danya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = nlp(doc)\n",
    "    tokens = [tokens.lower_ for tokens in doc]\n",
    "    tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
    "    tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
    "    final_token = [WordNetLemmatizer().lemmatize(token.text) for token in tokens]\n",
    "\n",
    "    return \" \".join(final_token)\n",
    "\n",
    "# function to preprocess speech\n",
    "# def clean(text):\n",
    "\n",
    "#     # removing paragraph numbers\n",
    "#     text = re.sub('[0-9]+.\\t', '', str(text))\n",
    "#     # removing new line characters\n",
    "#     text = re.sub('\\n ', '', str(text))\n",
    "#     text = re.sub('\\n', ' ', str(text))\n",
    "#     # removing apostrophes\n",
    "#     text = re.sub(\"'s\", '', str(text))\n",
    "#     # removing hyphens\n",
    "#     text = re.sub(\"-\", ' ', str(text))\n",
    "#     text = re.sub(\"â€” \", '', str(text))\n",
    "#     # removing quotation marks\n",
    "#     text = re.sub('\\\"', '', str(text))\n",
    "#     # removing salutations\n",
    "#     text = re.sub(\"Mr\\.\", 'Mr', str(text))\n",
    "#     text = re.sub(\"Mrs\\.\", 'Mrs', str(text))\n",
    "#     # removing any reference to outside text\n",
    "#     text = re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", str(text))\n",
    "\n",
    "#     return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split sentences\n",
    "def sentences(text):\n",
    "    # split sentences and questions\n",
    "    text = re.split('[.?]', text)\n",
    "    clean_sent = []\n",
    "    for sent in text:\n",
    "        clean_sent.append(sent)\n",
    "    return clean_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset \n",
    "df = pd.read_csv('Alldata_refined.csv')\n",
    "# Droping NULL rows\n",
    "df = df.dropna()\n",
    "# Extracting location col\n",
    "df = df[\"Locations\"]\n",
    "# Converting Data frame to sorted list in lower case\n",
    "Data_of_region = df.values.tolist()\n",
    "Data_of_region = [each_city.lower() for each_city in Data_of_region]\n",
    "Data_of_region = list(dict.fromkeys(sorted(Data_of_region)))\n",
    "# Storing indexes of each alphabet starting index \n",
    "index = dict()\n",
    "# Helping variables to store indexes \n",
    "flag= False\n",
    "push = False\n",
    "current_alphabet = \"\"\n",
    "start = 0\n",
    "finish = 0\n",
    "# Creating index hash \n",
    "for i in range(len(Data_of_region)):\n",
    "    if i != 0 and Data_of_region[i][0] != Data_of_region[i][0]:\n",
    "        flag = True\n",
    "        push = True\n",
    "        finish = i-1\n",
    "    if push == True:\n",
    "        index[current_alphabet] = finish\n",
    "    if flag == False:\n",
    "        start = i\n",
    "        current_alphabet = Data_of_region[i][0]\n",
    "        index.__setitem__(current_alphabet, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_location(read_more,header):\n",
    "    header = header.lower()\n",
    "    header = clean(header)\n",
    "    header = header.split()\n",
    "    text = sentences(clean(read_more))\n",
    "    cities = dict()\n",
    "    for i in text:\n",
    "        doc = nlp(i)\n",
    "        for token in range(len(doc)):\n",
    "                if doc[token].pos_ == \"PROPN\":\n",
    "                    flag = False\n",
    "                    end = index[doc[token].text.lower()[0]]\n",
    "                    if end == index['a']:\n",
    "                        start = 0\n",
    "                    else:\n",
    "                        start = chr(ord(doc[token].text.lower()[0])-1)\n",
    "                        # print(start)\n",
    "                        while(True):\n",
    "                            if start in index: \n",
    "                                start = index[start]-1\n",
    "                                break\n",
    "                            else:\n",
    "                                start = chr(ord(start)-1)\n",
    "                    area_count = 0\n",
    "                    for areas in range(start,end):\n",
    "                        words = Data_of_region[areas].split()\n",
    "                        subtoken = token\n",
    "                        checker = []\n",
    "                        for iterator in range(len(words)):\n",
    "                            if subtoken + iterator < len(doc):\n",
    "                                if doc[subtoken+iterator].text.lower() == words[iterator]:\n",
    "                                    checker.append(1)\n",
    "                        if len(checker) == len(words):\n",
    "                            city = ' '.join(words)\n",
    "                            area_count = len(words[iterator])\n",
    "                            flag = True\n",
    "                            break\n",
    "                    if flag:\n",
    "                        match = False\n",
    "                        word1 = \"\"\n",
    "                        word2 = \"\"\n",
    "                        if token != 0:\n",
    "                            word1 = doc[token-1].text.lower()\n",
    "                        subtoken = token + area_count\n",
    "                        if subtoken + 1 < len(doc):\n",
    "                            word2 = doc[subtoken+1].text.lower()\n",
    "                        if word1 in header or word2 in header:\n",
    "                            match = True\n",
    "                        if city in cities:\n",
    "                                if match:\n",
    "                                    cities[city] += 3\n",
    "                                else:\n",
    "                                    cities[city] += 1\n",
    "                        else:\n",
    "                            if match:\n",
    "                                cities.__setitem__(city, 3)\n",
    "                            else:\n",
    "                                cities.__setitem__(city, 1)\n",
    "    return cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mehar': 2, 'balochistan': 1, 'khyber pakhtunkhwa': 1, 'dadu': 2, 'khairpur': 2, 'johi': 1, 'jamali': 1, 'islamabad': 1}\n"
     ]
    }
   ],
   "source": [
    "header = \"DADU / ISLAMABAD: Sindh braced for yet more flooding on Thursday as a surge of water flowed down the Indus river, leaving parts of Dadu district inundated.\"\n",
    "# header=\"123\"\n",
    "print(Get_location(open(\"text.txt\", \"r\", encoding='utf8').read(),header))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('DIP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fee64b6d10f71b2ad32d2aedca1959ec8ccaf90c3c6db5aa37e1a96c8621a02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
